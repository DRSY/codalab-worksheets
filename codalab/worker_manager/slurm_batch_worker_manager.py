import argparse
import logging
import os
import uuid
from .worker_manager import WorkerManager, WorkerJob
from pathlib import Path
import subprocess
from codalab.worker.bundle_state import State

logger = logging.getLogger(__name__)


class SlurmBatchWorkerManager(WorkerManager):
    NAME = 'slurm-batch'
    DESCRIPTION = 'Worker manager for submitting jobs using Slurm Batch'

    SRUN_COMMAND = 'srun'
    SBATCH_COMMAND = 'sbatch'
    SBATCH_PREFIX = '#SBATCH'

    @staticmethod
    def add_arguments_to_subparser(subparser):
        subparser.add_argument(
            '--job-definition-name',
            type=str,
            default='codalab-slurm-worker',
            help='Name for the job definitions that will be generated by this worker manager',
        )
        subparser.add_argument(
            '--nodelist', type=str, default='', help='The worker node to run jobs in'
        )
        subparser.add_argument(
            '--partition', type=str, default='jag-standard', help='Name of batch job queue to use'
        )
        subparser.add_argument(
            '--cpus', type=int, default=1, help='Default number of CPUs for each worker'
        )
        subparser.add_argument(
            '--gpus', type=int, default=1, help='Default number of GPUs for each worker'
        )
        subparser.add_argument(
            '--memory-mb', type=int, default=2048, help='Default memory (in MB) for each worker'
        )
        subparser.add_argument(
            '--dry-run',
            action='store_true',
            help='Whether to print out Slurm batch job definition without submitting to Slurm',
        )
        subparser.add_argument(
            '--user',
            type=str,
            default='root',
            help='User to run the Slurm Batch CodaLab Worker jobs as',
        )

    def __init__(self, args, codalab_client):
        super().__init__(args, codalab_client)

    def get_worker_jobs(self):
        """
        Return a list of jobs.
        """
        # Get staged bundles
        jobs = []
        for state in State.ACTIVE_STATES + State.FINAL_STATES:
            keywords = ["state=" + state]
            if self.args.worker_tag:
                keywords.append('request_queue=' + self.args.worker_tag)
            bundles = self.codalab_client.fetch(
                'bundles', params={'worksheet': None, 'keywords': keywords, 'include': ['owner']}
            )
            jobs.extend(bundles)

        logger.info(
            'Workers: {}'.format(
                ' '.join(job['uuid'] + ':' + job['state'] for job in jobs) or '(none)'
            )
        )
        return [WorkerJob(job['state'] in State.ACTIVE_STATES) for job in jobs]

    def start_worker_job(self):
        """
        Start a Slurm worker job
        """
        image = 'codalab/worker:' + os.environ.get('CODALAB_VERSION', 'latest')
        worker_id = uuid.uuid4().hex
        # user's local home directory for easy acccess
        work_dir = os.path.join(str(Path.home()), "slurm-worker-scratch/{}".format(worker_id))
        logger.debug('Starting worker %s with image %s', worker_id, image)

        # This needs to be a unique directory since Batch jobs may share a host
        worker_network_prefix = 'cl_worker_{}_network'.format(worker_id)
        command = [
            'cl-worker',
            '--server',
            self.args.server,
            '--verbose',
            '--exit-when-idle',
            '--idle-seconds',
            str(self.args.worker_idle_seconds),
            '--work-dir',
            work_dir,
            '--id',
            worker_id,
            '--network-prefix',
            worker_network_prefix,
            # always set in Slurm worker manager to ensure safe shut down
            '--pass-down-termination',
        ]
        if self.args.worker_tag:
            command.extend(['--tag', self.args.worker_tag])
        slurm_args = self.map_codalab_args_to_slurm_args(self.args)
        sbatch_script = self.create_job_definition(slurm_args=slurm_args, command=command)

        # Not submit job to Slurm if dry run
        if self.dry_run:
            return

        self.save_job_definition(
            os.path.join(work_dir, self.args.job_definition_name + '.' + worker_id), sbatch_script
        )
        p = subprocess.Popen(
            [self.SBATCH_COMMAND, sbatch_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
        output, errors = p.communicate(timeout=60)
        logger.info(output.decode())

    def save_job_definition(self, job_file, sbatch_script_contents):
        """
        Save sbatch job definition to file.
        :param job_file: a file storing sbatch job configuration
        :param sbatch_script_contents: the contents of a sbatch job
        :return: 
        """
        with open(job_file, 'w') as f:
            f.write('Slurm CodaLab Worker Job Definition:')
            f.write(sbatch_script_contents)
        logger.info("Saved Slurm Batch Worker config file to {}".format(job_file))

    def create_job_definition(self, slurm_args, command):
        """
        Create a Slurm sbatch job definition structured as a list of sbatch arguments and a srun command
        :param slurm_args: arguments for launching a Slurm batch job
        :param command: arguments for starting a CodaLab worker
        :return:
        """
        sbatch_args = [
            '{} --{}={}'.format(self.SBATCH_PREFIX, key, slurm_args[key])
            for key in sorted(slurm_args.keys())
        ]
        srun_args = [self.SRUN_COMMAND, '--unbuffered'] + command
        # job definition contains two sections: sbatch arguments and srun command
        sbatch_script = '#!/bin/bash\n\n' + '\n'.join(sbatch_args) + '\n' + ' '.join(srun_args)
        print(sbatch_script)
        return sbatch_script

    def map_codalab_args_to_slurm_args(self, args):
        """
        Convert command line arguments to Slurm
        :param args: command line arguments
        :return: a dictionary of Slurm arguments
        """
        slurm_args = {}
        slurm_args['nodelist'] = args.nodelist
        slurm_args['mem-per-cpu'] = args.memory_mb
        slurm_args['partition'] = args.partition
        slurm_args['gres'] = "gpu:" + str(args.gpus)
        slurm_args['job-name'] = args.job_definition_name
        slurm_args['cpus-per-task'] = 3
        slurm_args['ntasks-per-node'] = 1
        slurm_args['time'] = '10-0'
        slurm_args["open-mode"] = 'append'
        return slurm_args
